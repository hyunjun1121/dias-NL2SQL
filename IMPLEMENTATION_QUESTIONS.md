# Implementation Questions and Confirmation Needed

## Date: 2025-10-11

êµ¬í˜„ ê³¼ì •ì—ì„œ ê²°ì •ì´ í•„ìš”í•˜ê±°ë‚˜ í™•ì¸ì´ í•„ìš”í•œ ë¶€ë¶„ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

---

## Category 1: Confidence Calculation

### Question 1.1: LLM Confidence Generation Format
**Current Implementation**:
```python
# LLMì—ê²Œ JSON formatìœ¼ë¡œ confidenceë¥¼ ìƒì„±í•˜ë„ë¡ ìš”ì²­
{
  "subtasks": [
    {"task_id": 1, "confidence": 0.95, "reasoning": "..."}
  ]
}
```

**Questions**:
1. **Temperature ì„¤ì •**: í˜„ì¬ temperature=0.0 (deterministic)ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
   - ì¥ì : Consistent confidence scores
   - ë‹¨ì : Exploration ë¶€ì¡±
   - **Confirm**: Temperature 0.0ì´ ë§ë‚˜ìš”? ì•„ë‹ˆë©´ 0.2-0.3 ì •ë„ë¡œ ì•½ê°„ì˜ variationì„ ì£¼ëŠ”ê²Œ ë‚˜ì„ê¹Œìš”?

2. **Confidence ë²”ìœ„**: 0.0-1.0 ë²”ìœ„ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
   - Alternative: 0-100 (percentage)
   - Alternative: Low/Medium/High (categorical)
   - **Confirm**: 0.0-1.0 ë²”ìœ„ê°€ ì ì ˆí•œê°€ìš”?

3. **Reasoning í•„ìˆ˜ ì—¬ë¶€**: í˜„ì¬ reasoningì„ í•„ìˆ˜ë¡œ ìš”êµ¬í•©ë‹ˆë‹¤.
   - ì¥ì : Interpretability, debugging ìš©ì´
   - ë‹¨ì : Token ì‚¬ìš©ëŸ‰ ì¦ê°€
   - **Confirm**: Reasoningì„ ê³„ì† ìš”êµ¬í• ê¹Œìš”, ì•„ë‹ˆë©´ optionalë¡œ ë§Œë“¤ê¹Œìš”?

### Question 1.2: Confidence Recalculation Timing
**Current Implementation**:
```python
# ê° task ì™„ë£Œ í›„ ë§¤ë²ˆ recalculate
for task in tasks:
    execute(task)
    recalculate_confidence(remaining_tasks, context)
```

**Questions**:
1. **Recalculation ë¹ˆë„**:
   - Current: ë§¤ taskë§ˆë‹¤
   - Alternative 1: Nê°œ taskë§ˆë‹¤ (e.g., 2ê°œë§ˆë‹¤)
   - Alternative 2: Rewardê°€ ë‚®ì„ ë•Œë§Œ
   - **Concern**: ë§¤ë²ˆ recalculateí•˜ë©´ API callì´ ë§ì•„ì§ (cost, latency)
   - **Confirm**: ë§¤ë²ˆ recalculateê°€ ë§ë‚˜ìš”?

2. **Context ì „ë‹¬ ë°©ì‹**:
   - Current: ëª¨ë“  completed tasksì˜ operation + result ì „ë‹¬
   - Alternative: Summaryë§Œ ì „ë‹¬ (e.g., "Completed: table selection, 1000 rows")
   - **Confirm**: Full context vs Summary?

---

## Category 2: SQL Generation

### Question 2.1: SQL Fragment vs Full SQL
**Current Implementation**:
```python
# Progressive: ê° taskë§ˆë‹¤ incremental SQL ìƒì„±
Iteration 1: "SELECT * FROM employees"
Iteration 2: "SELECT * FROM employees WHERE department='Engineering'"
Iteration 3: "... AND salary>50000"
```

**Questions**:
1. **Generation ë°©ì‹**:
   - Current: ì´ì „ SQLì„ extend
   - Alternative: ê° taskì˜ fragmentë¥¼ ìƒì„±í•˜ê³  ë‚˜ì¤‘ì— assemble
   - **Trade-off**:
     - Extend: Context ìœ ì§€, but ì—ëŸ¬ ëˆ„ì  ê°€ëŠ¥
     - Fragment: ë…ë¦½ì , but assembly logic í•„ìš”
   - **Confirm**: Extend ë°©ì‹ì´ ë§ë‚˜ìš”?

2. **Validation**: ê° iterationì˜ SQLì„ ë°”ë¡œ ì‹¤í–‰í•˜ëŠ”ë°, syntax errorê°€ ë‚˜ë©´?
   - Current: ê·¸ëƒ¥ ì‹¤íŒ¨ë¡œ ê¸°ë¡
   - Alternative: Syntax check í›„ ì¬ìƒì„± ì‹œë„
   - **Confirm**: Syntax error handling strategy?

### Question 2.2: SQL Optimization
**Current Implementation**:
- Optimization ì—†ìŒ (LLMì´ ìƒì„±í•œ ê·¸ëŒ€ë¡œ ì‚¬ìš©)

**Questions**:
1. **Query Optimization**:
   - ì˜ˆ: `SELECT *` â†’ `SELECT id, name, salary` (í•„ìš”í•œ columnë§Œ)
   - ì˜ˆ: Filter order optimization
   - **Concern**: Optimizationì´ semantic correctnessë¥¼ í•´ì¹  ìˆ˜ ìˆìŒ
   - **Confirm**: Optimizationì„ ì¶”ê°€í• ê¹Œìš”, ì•„ë‹ˆë©´ LLMì—ê²Œ ë§¡ê¸¸ê¹Œìš”?

---

## Category 3: Reward Calculation

### Question 3.1: LLM Semantic Judgment
**Current Implementation**:
```python
# Binary approach
# Step 1: Check execution success
# Step 2: LLM judges semantic correctness (if execution succeeded)
# Reward: 1.0 if both pass, 0.0 otherwise
```

**Questions**:
1. **LLM Judgment Temperature**: í˜„ì¬ temperature=0.0 (deterministic)ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
   - ì¥ì : Consistent judgments
   - ë‹¨ì : No variation in edge cases
   - **Confirm**: Temperature 0.0ì´ ë§ë‚˜ìš”?

2. **Prompt Design**: í˜„ì¬ 6ê°€ì§€ ê³ ë ¤ì‚¬í•­ì„ í¬í•¨í•œ prompt ì‚¬ìš©
   - í…Œì´ë¸”, ì»¬ëŸ¼, ì¡°ì¸, í•„í„°, ì§‘ê³„, ê²°ê³¼ íƒ€ë‹¹ì„±
   - **Confirm**: Promptì— ë” ì¶”ê°€í•˜ê±°ë‚˜ ì œê±°í•  ë‚´ìš©ì´ ìˆë‚˜ìš”?

3. **Reasoning í™œìš©**: LLMì´ ì œê³µí•œ reasoningì„ ì–´ë–»ê²Œ í™œìš©í• ê¹Œìš”?
   - ë‹¨ìˆœ ê¸°ë¡ë§Œ?
   - Error recoveryì— í™œìš©?
   - **Confirm**: Reasoningì„ ë‹¤ìŒ iterationì— ë°˜ì˜í• ê¹Œìš”?

### Question 3.2: Ground Truth Comparison
**Current Implementation**:
- Ground truth ì—†ì´ heuristicìœ¼ë¡œ í‰ê°€
- ì˜ˆ: Table nameì´ queryì— ë‚˜ì˜¤ë©´ correct

**Questions**:
1. **Ground Truth Schema Links**:
   - `remaining_tasks.md`ì— ê¸°ë¡í–ˆë“¯ì´, ground truth í™•ë³´ ë°©ì•ˆ ë¯¸ì •
   - **Options**:
     - Gold SQLì„ parseí•´ì„œ ì¶”ì¶œ
     - Manual annotation
     - ì—†ì´ ì§„í–‰ (heuristicë§Œ ì‚¬ìš©)
   - **Confirm**: ë‹¹ì¥ ì–´ë–»ê²Œ ì§„í–‰í• ê¹Œìš”?

2. **Evaluation ê¸°ì¤€**:
   - Current: NL queryì™€ schemaë§Œ ë³´ê³  íŒë‹¨
   - Alternative: Gold SQLê³¼ ë¹„êµ
   - **Confirm**: Gold SQL comparisonì„ êµ¬í˜„í•´ì•¼ í•˜ë‚˜ìš”?

---

## Category 4: Error Handling

### Question 4.1: Error Recovery Strategy
**Current Implementation**:
```python
if execution_result['success'] == False:
    # Just record the error, move to next iteration
    error_analysis = error_analyzer.analyze(...)
```

**Questions**:
1. **Retry Strategy**:
   - Current: No retry (í•œ ë²ˆ ì‹¤íŒ¨í•˜ë©´ ê·¸ëƒ¥ ì§„í–‰)
   - Alternative 1: ê°™ì€ taskë¥¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„
   - Alternative 2: Taskë¥¼ ë¶„í•´í•´ì„œ ë” ì‘ì€ taskë¡œ
   - **Confirm**: Retryë¥¼ êµ¬í˜„í•´ì•¼ í•˜ë‚˜ìš”?

2. **Error Feedback to LLM**:
   - Current: Error messageë¥¼ ë‹¨ìˆœíˆ ê¸°ë¡
   - Alternative: Error messageë¥¼ ë‹¤ìŒ generationì— í™œìš©
   - **Example**:
     ```python
     prompt += f"Previous attempt failed: {error_msg}. Try a different approach."
     ```
   - **Confirm**: Error feedback loopë¥¼ êµ¬í˜„í•´ì•¼ í•˜ë‚˜ìš”?

### Question 4.2: Multi-branch Reasoning
**Current Implementation**:
- ë‹¨ìˆœ êµ¬í˜„ (single branchë§Œ)
- `BranchCollection` data structureëŠ” ìˆì§€ë§Œ ì‚¬ìš© ì•ˆ í•¨

**Questions**:
1. **Multi-branch ìš°ì„ ìˆœìœ„**:
   - **Kyungmin mentioned**: MS rStar ìŠ¤íƒ€ì¼ multi-branch
   - Current: Not implemented
   - **Confirm**: ì§€ê¸ˆ ë‹¹ì¥ êµ¬í˜„í•´ì•¼ í•˜ë‚˜ìš”, ì•„ë‹ˆë©´ ë‚˜ì¤‘ì—?

2. **Branch Creation ì‹œì **:
   - Option 1: Error ë°œìƒ ì‹œ
   - Option 2: Rewardê°€ ë‚®ì„ ë•Œ (e.g., < 0.5)
   - Option 3: ì²˜ìŒë¶€í„° multiple branches ìœ ì§€
   - **Confirm**: ì–´ëŠ ì‹œì ì— branchë¥¼ ë§Œë“¤ì–´ì•¼ í•˜ë‚˜ìš”?

---

## Category 5: Performance and Efficiency

### Question 5.1: API Call Optimization
**Current Implementation**:
```python
# API calls per query:
# 1. Sub-task extraction (1 call)
# 2. Query plan generation (1 call)
# 3. SQL generation per task (N calls, N=avg 3-5)
# 4. Confidence recalculation per task (N calls)
# Total: ~10-15 calls per query
```

**Questions**:
1. **Call Reduction**:
   - **Concern**: ë„ˆë¬´ ë§ì€ API call â†’ high cost, high latency
   - **Options**:
     - Batch multiple tasks in one call
     - Skip confidence recalculation
     - Cache LLM responses
   - **Confirm**: Cost/latencyê°€ ë¬¸ì œê°€ ë ê¹Œìš”? Optimizationì´ í•„ìš”í•œê°€ìš”?

2. **Parallel Execution**:
   - Current: Sequential (í•œ taskì”©)
   - Alternative: Independent tasksë¥¼ parallelë¡œ ì‹¤í–‰
   - **Example**: Task 2ì™€ Task 3ê°€ ëª¨ë‘ Task 1ì—ë§Œ dependí•˜ë©´, 2ì™€ 3ì„ parallelë¡œ
   - **Confirm**: Parallel executionì„ êµ¬í˜„í• ê¹Œìš”?

### Question 5.2: Caching Strategy
**Current Implementation**:
- No caching

**Questions**:
1. **Cache Target**:
   - Schema embedding
   - Sub-task extraction results (for similar queries)
   - Query plan results
   - **Confirm**: ë¬´ì—‡ì„ cacheí•´ì•¼ í• ê¹Œìš”?

2. **Cache Invalidation**:
   - Schema ë³€ê²½ ì‹œ
   - Query ë³€ê²½ ì‹œ
   - **Confirm**: Cache strategyê°€ í•„ìš”í•œê°€ìš”?

---

## Category 6: Evaluation and Metrics

### Question 6.1: Evaluation Benchmark
**Current Implementation**:
- BIRD, Spider v2 ì¤€ë¹„ë¨
- Evaluation script: `scripts/run_pipeline.py`

**Questions**:
1. **Evaluation Split**:
   - BIRD dev set: ~500 examples
   - **Concern**: ì „ì²´ë¥¼ ë‹¤ ëŒë¦¬ë©´ ì‹œê°„/ë¹„ìš©ì´ ë§ì´ ë“¦
   - **Options**:
     - Subset (e.g., 50-100 examples)
     - Full evaluation
     - Stratified sampling
   - **Confirm**: ì²˜ìŒì— ëª‡ ê°œë¡œ í…ŒìŠ¤íŠ¸í• ê¹Œìš”?

2. **Evaluation Metrics**:
   - Current implementation:
     - Execution Accuracy (binary)
     - Semantic Correctness (0-1)
     - Total Reward (0-1)
   - **Missing**:
     - Exact match accuracy?
     - Component-wise accuracy (table/column/join)?
     - Error breakdown by type?
   - **Confirm**: ì–´ë–¤ metricì„ ì¶”ê°€ë¡œ ê³„ì‚°í•´ì•¼ í•˜ë‚˜ìš”?

### Question 6.2: Baseline Comparison
**Current Implementation**:
- Baselineê³¼ì˜ ë¹„êµ ì—†ìŒ

**Questions**:
1. **Comparison Target**:
   - ê¸°ì¡´ 6ê°œ êµ¬í˜„ (RASL, PNEUMA, etc.)ê³¼ ë¹„êµí•´ì•¼ í•˜ë‚˜ìš”?
   - GPT-4o baseline (zero-shot, few-shot)?
   - **Confirm**: ì–´ë–¤ baselineê³¼ ë¹„êµí• ê¹Œìš”?

2. **Ablation Study**:
   - Progressive execution vs one-shot
   - With confidence recalculation vs without
   - 60/20/20 reward vs execution-only
   - **Confirm**: Ablation studyë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?

---

## Category 7: Code Quality and Maintenance

### Question 7.1: Type Hints and Validation
**Current Implementation**:
- Partial type hints
- Minimal input validation

**Questions**:
1. **Type Checking**:
   - Add full type hints everywhere?
   - Use mypy for type checking?
   - **Confirm**: Type safetyê°€ ì¤‘ìš”í•œê°€ìš”?

2. **Input Validation**:
   - Schema validation
   - Query validation
   - Config validation
   - **Confirm**: ì–´ëŠ ì •ë„ validationì´ í•„ìš”í•œê°€ìš”?

### Question 7.2: Testing Strategy
**Current Implementation**:
- No unit tests yet

**Questions**:
1. **Test Coverage**:
   - Unit tests for each component?
   - Integration tests for pipeline?
   - End-to-end tests on BIRD samples?
   - **Confirm**: ì–´ë–¤ testê°€ ìš°ì„ ìˆœìœ„ì¸ê°€ìš”?

2. **Mock vs Real LLM**:
   - Mock LLM responses for fast testing?
   - Real LLM calls for accurate testing?
   - **Confirm**: Testing strategyëŠ”?

---

## Category 8: Configuration and Hyperparameters

### Question 8.1: Hyperparameter Tuning
**Current Implementation**:
```python
high_confidence_threshold = 0.85
acceptance_threshold = 0.7
max_iterations = 10
semantic_weight = 0.6
# ... etc
```

**Questions**:
1. **Tuning ë°©ë²•**:
   - Manual tuning on dev set?
   - Grid search?
   - Bayesian optimization?
   - **Confirm**: Hyperparameter tuningì„ ì–´ë–»ê²Œ í• ê¹Œìš”?

2. **Dataset-specific Config**:
   - BIRD vs Spider v2 ë‹¤ë¥¸ config?
   - Simple vs complex query ë‹¤ë¥¸ config?
   - **Confirm**: Dataset/query typeë³„ configë¥¼ ì¤€ë¹„í•´ì•¼ í•˜ë‚˜ìš”?

### Question 8.2: LLM Model Selection
**Current Implementation**:
- Default: GPT-4o
- Also supports: Claude

**Questions**:
1. **Model Comparison**:
   - GPT-4o vs Claude-3.5-Sonnet ì„±ëŠ¥ ë¹„êµ?
   - DeepSeek-R1 ì§€ì› ì¶”ê°€?
   - **Confirm**: ì–´ë–¤ ëª¨ë¸ë¡œ main evaluationì„ í• ê¹Œìš”?

2. **Cost vs Performance**:
   - GPT-4o: Expensive but accurate
   - GPT-4o-mini: Cheaper but less accurate
   - **Confirm**: Cost budgetì´ ìˆë‚˜ìš”?

---

## Category 9: Integration with Existing Work

### Question 9.1: ê¸°ì¡´ 6ê°œ êµ¬í˜„ í™œìš©
**Current Implementation**:
- ê°œë…ì ìœ¼ë¡œë§Œ ì°¸ê³ 
- ì‹¤ì œ ì½”ë“œ ì¬ì‚¬ìš© ì—†ìŒ

**Questions**:
1. **Component ì¬ì‚¬ìš©**:
   - RSL-SQLì˜ BSLì„ schema linkingì— ì‚¬ìš©?
   - Reward-SQLì˜ PRMì„ reward calculationì— ì‚¬ìš©?
   - LinkAlignì˜ multi-agentë¥¼ alternative generationì— ì‚¬ìš©?
   - **Confirm**: ê¸°ì¡´ êµ¬í˜„ì„ ë” ì§ì ‘ì ìœ¼ë¡œ í™œìš©í• ê¹Œìš”?

2. **Ensemble Approach**:
   - ì—¬ëŸ¬ ë°©ë²•ì„ combineí•´ì„œ ì‚¬ìš©?
   - Voting mechanism?
   - **Confirm**: Ensembleì´ í•„ìš”í•œê°€ìš”?

---

## Category 10: Deployment and Usage

### Question 10.1: Production Readiness
**Current Implementation**:
- Research prototype
- No production features

**Questions**:
1. **Production Features**:
   - Logging and monitoring?
   - Error reporting?
   - Rate limiting?
   - **Confirm**: Productionì„ ê³ ë ¤í•´ì•¼ í•˜ë‚˜ìš”, ì•„ë‹ˆë©´ research only?

2. **API Service**:
   - REST APIë¡œ wrapping?
   - Batch processing support?
   - **Confirm**: Service í˜•íƒœë¡œ deployí•  ê³„íšì´ ìˆë‚˜ìš”?

---

## Priority Ranking

ì œê°€ ìƒê°í•˜ëŠ” ìš°ì„ ìˆœìœ„ë³„ ì •ë¦¬:

### ğŸ”´ High Priority (ì¦‰ì‹œ ê²°ì • í•„ìš”)
1. **Confidence Recalculation ë¹ˆë„** (Category 1.2)
   - API cost/latencyì— ì§ì ‘ ì˜í–¥
2. **Ground Truth Comparison** (Category 3.2)
   - Evaluation ì •í™•ë„ì— ì˜í–¥
3. **Evaluation Subset Size** (Category 6.1)
   - ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ì— í•„ìš”
4. **Error Recovery Strategy** (Category 4.1)
   - í˜„ì¬ failure handlingì´ ë„ˆë¬´ ë‹¨ìˆœ

### ğŸŸ¡ Medium Priority (ê°œì„  ì‹œ ê³ ë ¤)
5. **LLM Semantic Judgment Prompt** (Category 3.1)
   - Prompt ê°œì„ ìœ¼ë¡œ judgment ì •í™•ë„ í–¥ìƒ
6. **Multi-branch Implementation** (Category 4.2)
   - Kyungminì´ ì–¸ê¸‰í–ˆì§€ë§Œ ë‹¹ì¥ì€ ì•„ë‹ ìˆ˜ë„
7. **API Call Optimization** (Category 5.1)
   - Cost ì ˆê°
8. **Hyperparameter Tuning** (Category 8.1)
   - ì„±ëŠ¥ ìµœì í™”

### ğŸŸ¢ Low Priority (ë‚˜ì¤‘ì— ê³ ë ¤)
9. **Type Hints** (Category 7.1)
   - Code quality
10. **Production Features** (Category 10.1)
    - Research â†’ Production ì „í™˜ ì‹œ

---

## Decisions Needed From You

ë‹¤ìŒ ì‚¬í•­ë“¤ì— ëŒ€í•´ ê²°ì •/í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤:

1. **Confidence recalculation**: ë§¤ë²ˆ? Në²ˆë§ˆë‹¤? Reward ë‚®ì„ ë•Œë§Œ?
2. **Ground truth**: ì§€ê¸ˆ ë‹¹ì¥ ì–´ë–»ê²Œ í™•ë³´? ì•„ë‹ˆë©´ heuristicë§Œ?
3. **Error recovery**: Retry êµ¬í˜„? ì•„ë‹ˆë©´ ë‹¨ìˆœíˆ ê¸°ë¡ë§Œ?
4. **Multi-branch**: ì§€ê¸ˆ êµ¬í˜„? ë‚˜ì¤‘ì—?
5. **Evaluation size**: BIRD dev set ëª‡ ê°œë¡œ ì‹œì‘?
6. **Baseline comparison**: í•„ìš”? ì–´ë–¤ baseline?
7. **Hyperparameter tuning**: Manual? Automated?
8. **Model selection**: GPT-4o? Claude? ë‘˜ ë‹¤?

---

**Date**: 2025-10-11
**Status**: Awaiting decisions on priority questions
