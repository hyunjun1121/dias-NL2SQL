# Complete NL2SQL Pipeline Overview

## ğŸ“‹ Complete Pipeline Stages (0-5)

### **Stage 0: CHESS Information Retrieval (IR)** ğŸ”
**Component**: `ir/ir_integration.py`

#### Purpose
Schema pruning - ëŒ€ê·œëª¨ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ í…Œì´ë¸”/ì»¬ëŸ¼ë§Œ ì¶”ì¶œ

#### Process
1. **Extract Keywords**
   - LLMì´ ìì—°ì–´ ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
   - Template-based prompt

2. **Retrieve Entity**
   - í‚¤ì›Œë“œì™€ ë§¤ì¹­ë˜ëŠ” í…Œì´ë¸”/ì»¬ëŸ¼ ì°¾ê¸°
   - Vector similarity search (ChromaDB)

3. **Retrieve Context**
   - Top-K ê´€ë ¨ ì»¬ëŸ¼ ì„ íƒ
   - ì˜ˆì‹œ ê°’ê³¼ ì„¤ëª… í¬í•¨

#### Input
```python
{
    "question": "Show employees with salary over 50000 in Engineering",
    "db_id": "company",
    "full_schema": {
        "employees": [...],
        "departments": [...],
        "projects": [...],
        # ... 100+ tables
    }
}
```

#### Output (Pruned Schema)
```python
{
    "employees": {
        "columns": [
            {"name": "id"},
            {"name": "name"},
            {"name": "salary"},
            {"name": "department"}
        ]
    }
}
```

#### Why Important?
- ëŒ€ê·œëª¨ DB (100+ tables)ì—ì„œ ê´€ë ¨ í…Œì´ë¸”ë§Œ ì„ íƒ
- LLM context ì ˆì•½
- SQL ìƒì„± ì •í™•ë„ í–¥ìƒ

---

### **Stage 1: Confident Sub-task Extraction** ğŸ¯
**Component**: `model/subtask_extractor.py`

#### Purpose
ìì—°ì–´ ì¿¼ë¦¬ë¥¼ atomic sub-tasksë¡œ ë¶„í•´ + confidence score ìƒì„±

#### Input
- Pruned schema (from Stage 0)
- Natural language question

#### Output
```python
[
    SubTask(id=1, conf=0.95, op="SELECT FROM employees"),
    SubTask(id=2, conf=0.92, op="WHERE department='Engineering'"),
    SubTask(id=3, conf=0.90, op="WHERE salary>50000")
]
```

---

### **Stage 2: Query Plan Generation** ğŸ“
**Component**: `model/query_plan_generator.py`

#### Purpose
Human-readable 3-step query plan (CHASE-SQL methodology)

#### Output
```python
QueryPlan(
    steps=[
        Step(1, "find_tables", "Find employees table"),
        Step(2, "perform_operations", "Filter by dept and salary"),
        Step(3, "select_columns", "Return all columns")
    ]
)
```

---

### **Stage 3: Progressive Execution Loop** ğŸ”„
**Component**: `model/progressive_executor.py`

#### Kyungmin's Core Innovation
- Execute highest confidence task first
- **Immediate execution** (not deferred)
- Accumulate context from results
- Recalculate remaining task confidence

#### Process
```python
for iteration in range(max_iterations):
    task = get_highest_confidence_task()
    sql = generate_sql_fragment(task, context)
    result = db_executor.execute(sql)  # Execute immediately!

    if result.success:
        reward = calculate_semantic_reward(sql, result)
        if reward > threshold:
            context.update(task, result)
```

---

### **Stage 4: Semantic Reward Evaluation** â­
**Component**: `model/semantic_reward.py`

#### Binary Approach (Simplified)
```python
if execution_fails:
    reward = 0.0
elif LLM_judges_semantic_as_incorrect:
    reward = 0.0
else:
    reward = 1.0  # Perfect!
```

#### LLM Judgment
```
CORRECT: YES/NO
REASONING: [Detailed explanation]
```

---

### **Stage 5: Error Analysis** ğŸ›
**Component**: `evaluation/error_analyzer.py`

#### Error Priority
1. **High**: Semantic errors (wrong table, wrong column, wrong join)
2. **Low**: Syntax errors (easily fixable)
3. **Medium**: Execution errors (timeout, invalid ops)

---

## ğŸ”§ LLM Integration Points

### Current LLMClient Architecture
**File**: `utils/llm_client.py`

#### Supported Backends
- OpenAI (GPT-4o)
- Anthropic (Claude)
- vLLM cluster
- HuggingFace Inference API
- Ollama
- **Transformers (local)** â† ì—¬ê¸°ì— ìƒˆ ëª¨ë¸ ì¶”ê°€!

#### Stage-specific LLM Usage
```python
class EPFLHyunjunPipeline:
    def __init__(self, config):
        # Each stage can use different models!
        self.subtask_llm = LLMClient(config.subtask.model_name)
        self.plan_llm = LLMClient(config.query_plan.model_name)
        self.sql_llm = LLMClient(config.progressive_execution.sql_model_name)
        self.reward_llm = LLMClient(config.semantic_reward.model_name)
```

---

## ğŸ¯ ëª¨ë¸ í†µí•© ì „ëµ

### Tested Models Performance (NL2SQL)

| Model | Memory | SQL Quality | Speed | Best For |
|-------|--------|-------------|-------|----------|
| **Qwen3-480B-Coder** | 340GB | âœ… Perfect SQL | 259s | **SQL Generation** |
| **Qwen3-235B-Thinking** | 220GB | âŒ No SQL (thinking only) | 77s | Sub-task reasoning |
| **MiniMax-M2** | 214GB | âŒ No SQL (thinking only) | 53s | Sub-task reasoning |

### Recommended Pipeline Configuration

```python
# Stage 0: IR (keyword extraction)
ir_model = "gpt-4o-mini"  # Fast & cheap for keyword extraction

# Stage 1: Sub-task Extraction
subtask_model = "Qwen3-235B-Thinking"  # or MiniMax-M2
# Benefit: Detailed reasoning about sub-tasks

# Stage 2: Query Plan
plan_model = "Qwen3-235B-Thinking"  # or MiniMax-M2
# Benefit: Step-by-step planning

# Stage 3: SQL Generation (CRITICAL!)
sql_model = "Qwen3-480B-Coder"
# Benefit: Specialized for code/SQL, produces complete queries

# Stage 4: Semantic Reward
reward_model = "gpt-4o-mini"  # or any thinking model
# Benefit: Just needs to judge correctness
```

### Why Qwen3-480B-Coder is Essential

1. **Only model that generates complete SQL** âœ…
2. **No thinking tags** - direct SQL output
3. **Code-specialized** - understands SQL syntax perfectly
4. **Works on 4 GPUs** - practical deployment

### Integration Steps

1. **Update LLMClient** (`utils/llm_client.py`)
   ```python
   def _detect_backend(self):
       if model_lower in ["qwen3-480b-coder", "qwen3-235b-thinking", "minimax-m2"]:
           return "transformers_local"
   ```

2. **Add Local Transformers Handler**
   ```python
   elif self.backend == "transformers_local":
       return self._load_qwen_model()
   ```

3. **Update Config** (`config/config.py`)
   ```python
   class ProgressiveExecutionConfig:
       sql_model_name: str = "qwen3-480b-coder"
   ```

---

## ğŸ“Š Pipeline Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT: Natural Language Query + Database Path          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 0: CHESS IR (Schema Pruning)                     â”‚
â”‚ â”œâ”€ Extract Keywords (LLM)                              â”‚
â”‚ â”œâ”€ Retrieve Entity (Vector DB)                         â”‚
â”‚ â””â”€ Retrieve Context (Top-K)                            â”‚
â”‚ Output: Pruned Schema                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: Sub-task Extraction                           â”‚
â”‚ LLM: Qwen3-235B-Thinking / MiniMax-M2                 â”‚
â”‚ Output: [Task1(0.95), Task2(0.92), Task3(0.90)]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2: Query Plan Generation                         â”‚
â”‚ LLM: Qwen3-235B-Thinking / MiniMax-M2                 â”‚
â”‚ Output: 3-step plan                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 3: Progressive Execution (LOOP)                  â”‚
â”‚ LLM: **Qwen3-480B-Coder** â† CRITICAL!                 â”‚
â”‚                                                          â”‚
â”‚ For each task (highest confidence first):              â”‚
â”‚   1. Generate SQL fragment                             â”‚
â”‚   2. Execute immediately                               â”‚
â”‚   3. Calculate reward                                  â”‚
â”‚   4. Update context if good                            â”‚
â”‚                                                          â”‚
â”‚ Output: Final SQL                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 4: Semantic Reward Evaluation                    â”‚
â”‚ LLM: Any thinking model (GPT-4o-mini)                  â”‚
â”‚                                                          â”‚
â”‚ Binary Decision:                                        â”‚
â”‚   Execution OK? + Semantically Correct? â†’ 1.0 : 0.0   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 5: Error Analysis (if reward = 0)                â”‚
â”‚ Categorize: Semantic > Execution > Syntax              â”‚
â”‚ Suggest fixes                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OUTPUT: PipelineOutput                                  â”‚
â”‚ â”œâ”€ final_sql: str                                      â”‚
â”‚ â”œâ”€ execution_result: Dict                              â”‚
â”‚ â”œâ”€ semantic_correctness: bool                          â”‚
â”‚ â”œâ”€ total_reward: 1.0 or 0.0                           â”‚
â”‚ â””â”€ execution_time: float                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Next Steps

1. **Extend LLMClient** to support local Qwen models
2. **Test Stage 3** with Qwen3-480B-Coder
3. **Benchmark** against baseline (GPT-4o)
4. **Optimize** memory management for 4-GPU setup
5. **Deploy** on HPC cluster

---

**Created**: 2025-11-15
**Status**: Ready for Qwen model integration